#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
============================================================================
JOB 5: SISTEMA DE RECOMENDACI√ìN CON SPARK ALS
============================================================================
Objetivo: Entrenar modelo ALS para recomendar canciones a usuarios
Input: listening_clean (Parquet en S3)
Output: Modelo ALS + recomendaciones + m√©tricas
============================================================================
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, stddev, min as spark_min, max as spark_max
from pyspark.sql.types import IntegerType
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
import sys

# ============================================================================
# CONFIGURACI√ìN DE SPARK
# ============================================================================

print("=" * 80)
print("INICIANDO JOB 5: SISTEMA DE RECOMENDACI√ìN")
print("=" * 80)

spark = SparkSession.builder \
    .appName("MAESTRO-Job5-Recommender-ALS") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Paths en S3
S3_BUCKET = "s3://emr-logs-1758750407/music-data"
INPUT_LISTENING = f"{S3_BUCKET}/cleaned/listening/"
OUTPUT_MODEL = f"{S3_BUCKET}/models/als_model/"
OUTPUT_RECOMMENDATIONS = f"{S3_BUCKET}/recommendations/"
OUTPUT_METRICS = f"{S3_BUCKET}/metrics/"

print("\nüìÇ Configuraci√≥n de rutas:")
print(f"   Input: {INPUT_LISTENING}")
print(f"   Modelo: {OUTPUT_MODEL}")
print(f"   Recomendaciones: {OUTPUT_RECOMMENDATIONS}")


# ============================================================================
# PASO 1: CARGAR Y EXPLORAR DATOS
# ============================================================================

print("\n" + "=" * 80)
print("PASO 1: CARGA Y EXPLORACI√ìN DE DATOS")
print("=" * 80)

# Cargar listening_clean
df_listening = spark.read.parquet(INPUT_LISTENING)

print(f"\n‚úì Datos cargados: {df_listening.count():,} registros")
print("\nEsquema:")
df_listening.printSchema()

# Estad√≠sticas b√°sicas
print("\nüìä Estad√≠sticas de interacciones:")
df_listening.select(
    count("*").alias("total_interactions"),
    count("user_id").alias("distinct_users"),
    count("track_id").alias("distinct_tracks"),
    avg("total_playcount").alias("avg_playcount"),
    stddev("total_playcount").alias("stddev_playcount"),
    spark_min("total_playcount").alias("min_playcount"),
    spark_max("total_playcount").alias("max_playcount")
).show()

# Distribuci√≥n de interacciones por usuario
print("\nüìà Distribuci√≥n de canciones por usuario:")
user_songs = df_listening.groupBy("user_id").agg(
    count("track_id").alias("num_songs"),
    avg("total_playcount").alias("avg_plays")
)
user_songs.describe().show()


# ============================================================================
# PASO 2: PREPARACI√ìN DE DATOS PARA ALS
# ============================================================================

print("\n" + "=" * 80)
print("PASO 2: PREPARACI√ìN DE DATOS")
print("=" * 80)

# ALS requiere IDs num√©ricos, as√≠ que creamos √≠ndices
from pyspark.ml.feature import StringIndexer

print("\n‚öôÔ∏è  Creando √≠ndices num√©ricos para usuarios y canciones...")

# Indexar usuarios
user_indexer = StringIndexer(inputCol="user_id", outputCol="user_idx")
user_model = user_indexer.fit(df_listening)
df_indexed = user_model.transform(df_listening)

# Indexar tracks
track_indexer = StringIndexer(inputCol="track_id", outputCol="track_idx")
track_model = track_indexer.fit(df_indexed)
df_indexed = track_model.transform(df_indexed)

# Seleccionar columnas necesarias y convertir a int
df_als = df_indexed.select(
    col("user_idx").cast(IntegerType()).alias("user"),
    col("track_idx").cast(IntegerType()).alias("item"),
    col("total_playcount").cast(IntegerType()).alias("rating")
)

print(f"‚úì Datos indexados: {df_als.count():,} registros")
print("\nMuestra de datos preparados:")
df_als.show(10)

# Filtrar usuarios/canciones con muy pocas interacciones (mejora calidad)
print("\nüîç Aplicando filtros de calidad...")

min_user_interactions = 5  # Usuario debe tener al menos 5 canciones
min_item_interactions = 5  # Canci√≥n debe tener al menos 5 usuarios

user_counts = df_als.groupBy("user").count().filter(col("count") >= min_user_interactions)
item_counts = df_als.groupBy("item").count().filter(col("count") >= min_item_interactions)

df_filtered = df_als.join(user_counts.select("user"), "user") \
                    .join(item_counts.select("item"), "item")

print(f"‚úì Despu√©s de filtrado: {df_filtered.count():,} registros")
print(f"   Usuarios √∫nicos: {df_filtered.select('user').distinct().count():,}")
print(f"   Canciones √∫nicas: {df_filtered.select('item').distinct().count():,}")


# ============================================================================
# PASO 3: DIVISI√ìN TRAIN/TEST
# ============================================================================

print("\n" + "=" * 80)
print("PASO 3: DIVISI√ìN TRAIN/TEST")
print("=" * 80)

# Split 80/20
(training, test) = df_filtered.randomSplit([0.8, 0.2], seed=42)

print(f"\nüìä Distribuci√≥n de datos:")
print(f"   Training: {training.count():,} registros ({training.count()/df_filtered.count()*100:.1f}%)")
print(f"   Test: {test.count():,} registros ({test.count()/df_filtered.count()*100:.1f}%)")


# ============================================================================
# PASO 4: ENTRENAMIENTO DEL MODELO ALS
# ============================================================================

print("\n" + "=" * 80)
print("PASO 4: ENTRENAMIENTO DEL MODELO ALS")
print("=" * 80)

print("\n‚öôÔ∏è  Configurando modelo ALS...")

# Configuraci√≥n del modelo
als = ALS(
    maxIter=10,
    rank=10,  # N√∫mero de factores latentes
    regParam=0.1,  # Regularizaci√≥n
    userCol="user",
    itemCol="item",
    ratingCol="rating",
    coldStartStrategy="drop",  # Ignorar usuarios/items nuevos en test
    implicitPrefs=False,  # Ratings expl√≠citos (playcount)
    nonnegative=True  # Factores no negativos
)

print("\nüöÄ Entrenando modelo ALS...")
print("   (esto puede tardar 3-5 minutos)")

model = als.fit(training)

print("‚úì Modelo entrenado exitosamente!")


# ============================================================================
# PASO 5: EVALUACI√ìN DEL MODELO
# ============================================================================

print("\n" + "=" * 80)
print("PASO 5: EVALUACI√ìN DEL MODELO")
print("=" * 80)

# Predicciones en test set
print("\nüìä Generando predicciones en test set...")
predictions = model.transform(test)
predictions_clean = predictions.filter(col("prediction").isNotNull())

print(f"‚úì Predicciones generadas: {predictions_clean.count():,}")

# Evaluar RMSE (Root Mean Square Error)
evaluator = RegressionEvaluator(
    metricName="rmse",
    labelCol="rating",
    predictionCol="prediction"
)

rmse = evaluator.evaluate(predictions_clean)
print(f"\nüìà RMSE del modelo: {rmse:.4f}")

# Evaluar MAE (Mean Absolute Error)
evaluator_mae = RegressionEvaluator(
    metricName="mae",
    labelCol="rating",
    predictionCol="prediction"
)
mae = evaluator_mae.evaluate(predictions_clean)
print(f"üìà MAE del modelo: {mae:.4f}")

# Guardar m√©tricas
metrics_data = [("RMSE", rmse), ("MAE", mae)]
df_metrics = spark.createDataFrame(metrics_data, ["metric", "value"])
df_metrics.coalesce(1).write.mode("overwrite").parquet(f"{OUTPUT_METRICS}/model_evaluation/")
print(f"\n‚úì M√©tricas guardadas en {OUTPUT_METRICS}/model_evaluation/")


# ============================================================================
# PASO 6: GENERAR RECOMENDACIONES
# ============================================================================

print("\n" + "=" * 80)
print("PASO 6: GENERACI√ìN DE RECOMENDACIONES")
print("=" * 80)

# Top 10 recomendaciones para cada usuario
print("\n‚öôÔ∏è  Generando top 10 recomendaciones por usuario...")
user_recs = model.recommendForAllUsers(10)
print(f"‚úì Recomendaciones generadas para {user_recs.count():,} usuarios")

# Guardar recomendaciones
user_recs.write.mode("overwrite").parquet(f"{OUTPUT_RECOMMENDATIONS}/user_recommendations/")
print(f"‚úì Guardadas en {OUTPUT_RECOMMENDATIONS}/user_recommendations/")

# Ejemplo de recomendaciones
print("\nüìã Ejemplo de recomendaciones (primeros 5 usuarios):")
user_recs.show(5, truncate=False)

# Top 10 usuarios similares para cada canci√≥n
print("\n‚öôÔ∏è  Generando top 10 usuarios por canci√≥n...")
item_recs = model.recommendForAllItems(10)
print(f"‚úì Recomendaciones generadas para {item_recs.count():,} canciones")

item_recs.write.mode("overwrite").parquet(f"{OUTPUT_RECOMMENDATIONS}/item_recommendations/")
print(f"‚úì Guardadas en {OUTPUT_RECOMMENDATIONS}/item_recommendations/")


# ============================================================================
# PASO 7: CASOS DE USO PR√ÅCTICOS
# ============================================================================

print("\n" + "=" * 80)
print("PASO 7: CASOS DE USO Y AN√ÅLISIS")
print("=" * 80)

# Recuperar mapeos originales
user_labels = user_model.labels
track_labels = track_model.labels

# Crear DataFrames con los mapeos
df_user_mapping = spark.createDataFrame(
    [(i, user_id) for i, user_id in enumerate(user_labels)],
    ["user_idx", "user_id"]
)

df_track_mapping = spark.createDataFrame(
    [(i, track_id) for i, track_id in enumerate(track_labels)],
    ["track_idx", "track_id"]
)

# Guardar mapeos para uso futuro
df_user_mapping.write.mode("overwrite").parquet(f"{OUTPUT_MODEL}/user_mapping/")
df_track_mapping.write.mode("overwrite").parquet(f"{OUTPUT_MODEL}/track_mapping/")
print("‚úì Mapeos guardados para uso posterior")

# Ejemplo: Recomendaciones para usuarios espec√≠ficos
print("\nüìå EJEMPLO: Recomendaciones para usuarios de muestra")

# Tomar 5 usuarios aleatorios
sample_users = df_filtered.select("user").distinct().limit(5)
sample_recs = model.recommendForUserSubset(sample_users, 5)

print("\nTop 5 recomendaciones para usuarios de muestra:")
sample_recs.show(truncate=False)


# ============================================================================
# PASO 8: ESTAD√çSTICAS FINALES
# ============================================================================

print("\n" + "=" * 80)
print("PASO 8: ESTAD√çSTICAS FINALES")
print("=" * 80)

# Calcular estad√≠sticas de las recomendaciones
print("\nüìä An√°lisis de recomendaciones:")

# Distribuci√≥n de scores de recomendaci√≥n
from pyspark.sql.functions import explode, col as F_col

user_recs_exploded = user_recs.select(
    F_col("user"),
    explode(F_col("recommendations")).alias("rec")
).select(
    F_col("user"),
    F_col("rec.item").alias("item"),
    F_col("rec.rating").alias("score")
)

print("\nDistribuci√≥n de scores de recomendaci√≥n:")
user_recs_exploded.select("score").describe().show()

# Guardar recomendaciones en formato legible
user_recs_exploded.write.mode("overwrite").parquet(
    f"{OUTPUT_RECOMMENDATIONS}/user_recs_exploded/"
)
print(f"‚úì Recomendaciones en formato expandido guardadas")


# ============================================================================
# RESUMEN FINAL
# ============================================================================

print("\n" + "=" * 80)
print("JOB 5 COMPLETADO EXITOSAMENTE")
print("=" * 80)

print(f"""
üìä RESUMEN DEL MODELO:
   ‚Ä¢ Algoritmo: ALS (Alternating Least Squares)
   ‚Ä¢ Factores latentes (rank): 10
   ‚Ä¢ Iteraciones: 10
   ‚Ä¢ RMSE: {rmse:.4f}
   ‚Ä¢ MAE: {mae:.4f}
   
üìÅ ARCHIVOS GENERADOS:
   ‚Ä¢ Modelo ALS: {OUTPUT_MODEL}
   ‚Ä¢ Recomendaciones por usuario: {OUTPUT_RECOMMENDATIONS}/user_recommendations/
   ‚Ä¢ Recomendaciones por canci√≥n: {OUTPUT_RECOMMENDATIONS}/item_recommendations/
   ‚Ä¢ M√©tricas: {OUTPUT_METRICS}/model_evaluation/
   ‚Ä¢ Mapeos: {OUTPUT_MODEL}/user_mapping/ y track_mapping/

‚úÖ El modelo est√° listo para generar recomendaciones personalizadas!
""")

spark.stop()
print("Spark session finalizada.")
